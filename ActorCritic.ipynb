{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3hl2RUEm0BY+P1TKuGQ0M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AWS_Reinforcement_Learning/blob/main/ActorCritic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nhFQ3-C2pdWo",
        "outputId": "6fe9c6c2-5502-4b55-f91b-7d732fd928ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "<ipython-input-1-efc71599e668>:48: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of episode :20, avg score : 24.0\n",
            "# of episode :40, avg score : 21.1\n",
            "# of episode :60, avg score : 23.6\n",
            "# of episode :80, avg score : 28.1\n",
            "# of episode :100, avg score : 33.6\n",
            "# of episode :120, avg score : 36.5\n",
            "# of episode :140, avg score : 57.5\n",
            "# of episode :160, avg score : 46.1\n",
            "# of episode :180, avg score : 57.3\n",
            "# of episode :200, avg score : 67.2\n",
            "# of episode :220, avg score : 73.5\n",
            "# of episode :240, avg score : 98.4\n",
            "# of episode :260, avg score : 164.4\n",
            "# of episode :280, avg score : 129.4\n",
            "# of episode :300, avg score : 205.2\n",
            "# of episode :320, avg score : 231.1\n",
            "# of episode :340, avg score : 235.3\n",
            "# of episode :360, avg score : 277.5\n",
            "# of episode :380, avg score : 272.9\n",
            "# of episode :400, avg score : 190.9\n",
            "# of episode :420, avg score : 198.7\n",
            "# of episode :440, avg score : 334.4\n",
            "# of episode :460, avg score : 249.1\n",
            "# of episode :480, avg score : 260.4\n",
            "# of episode :500, avg score : 301.8\n",
            "# of episode :520, avg score : 342.3\n",
            "# of episode :540, avg score : 344.2\n",
            "# of episode :560, avg score : 339.7\n",
            "# of episode :580, avg score : 336.6\n",
            "# of episode :600, avg score : 239.0\n",
            "# of episode :620, avg score : 273.6\n",
            "# of episode :640, avg score : 322.4\n",
            "# of episode :660, avg score : 387.3\n",
            "# of episode :680, avg score : 409.8\n",
            "# of episode :700, avg score : 371.3\n",
            "# of episode :720, avg score : 437.6\n",
            "# of episode :740, avg score : 476.2\n",
            "# of episode :760, avg score : 460.1\n",
            "# of episode :780, avg score : 492.2\n",
            "# of episode :800, avg score : 455.9\n",
            "# of episode :820, avg score : 396.1\n",
            "# of episode :840, avg score : 479.6\n",
            "# of episode :860, avg score : 467.6\n",
            "# of episode :880, avg score : 279.9\n",
            "# of episode :900, avg score : 405.4\n",
            "# of episode :920, avg score : 485.6\n",
            "# of episode :940, avg score : 487.6\n",
            "# of episode :960, avg score : 491.8\n",
            "# of episode :980, avg score : 350.9\n",
            "# of episode :1000, avg score : 463.4\n",
            "# of episode :1020, avg score : 489.8\n",
            "# of episode :1040, avg score : 486.9\n",
            "# of episode :1060, avg score : 500.0\n",
            "# of episode :1080, avg score : 496.1\n",
            "# of episode :1100, avg score : 500.0\n",
            "# of episode :1120, avg score : 500.0\n",
            "# of episode :1140, avg score : 484.3\n",
            "# of episode :1160, avg score : 426.1\n",
            "# of episode :1180, avg score : 477.2\n",
            "# of episode :1200, avg score : 500.0\n",
            "# of episode :1220, avg score : 500.0\n",
            "# of episode :1240, avg score : 500.0\n",
            "# of episode :1260, avg score : 490.1\n",
            "# of episode :1280, avg score : 500.0\n",
            "# of episode :1300, avg score : 439.4\n",
            "# of episode :1320, avg score : 500.0\n",
            "# of episode :1340, avg score : 500.0\n",
            "# of episode :1360, avg score : 500.0\n",
            "# of episode :1380, avg score : 500.0\n",
            "# of episode :1400, avg score : 500.0\n",
            "# of episode :1420, avg score : 500.0\n",
            "# of episode :1440, avg score : 500.0\n",
            "# of episode :1460, avg score : 500.0\n",
            "# of episode :1480, avg score : 497.3\n",
            "# of episode :1500, avg score : 500.0\n",
            "# of episode :1520, avg score : 500.0\n",
            "# of episode :1540, avg score : 500.0\n",
            "# of episode :1560, avg score : 475.1\n",
            "# of episode :1580, avg score : 389.2\n",
            "# of episode :1600, avg score : 421.4\n",
            "# of episode :1620, avg score : 500.0\n",
            "# of episode :1640, avg score : 500.0\n",
            "# of episode :1660, avg score : 499.6\n",
            "# of episode :1680, avg score : 500.0\n",
            "# of episode :1700, avg score : 500.0\n",
            "# of episode :1720, avg score : 498.9\n",
            "# of episode :1740, avg score : 500.0\n",
            "# of episode :1760, avg score : 500.0\n",
            "# of episode :1780, avg score : 500.0\n",
            "# of episode :1800, avg score : 498.1\n",
            "# of episode :1820, avg score : 500.0\n",
            "# of episode :1840, avg score : 500.0\n",
            "# of episode :1860, avg score : 500.0\n",
            "# of episode :1880, avg score : 500.0\n",
            "# of episode :1900, avg score : 500.0\n",
            "# of episode :1920, avg score : 500.0\n",
            "# of episode :1940, avg score : 500.0\n",
            "# of episode :1960, avg score : 500.0\n",
            "# of episode :1980, avg score : 491.4\n",
            "# of episode :2000, avg score : 437.1\n",
            "# of episode :2020, avg score : 500.0\n",
            "# of episode :2040, avg score : 500.0\n",
            "# of episode :2060, avg score : 500.0\n",
            "# of episode :2080, avg score : 500.0\n",
            "# of episode :2100, avg score : 500.0\n",
            "# of episode :2120, avg score : 500.0\n",
            "# of episode :2140, avg score : 500.0\n",
            "# of episode :2160, avg score : 500.0\n",
            "# of episode :2180, avg score : 500.0\n",
            "# of episode :2200, avg score : 500.0\n",
            "# of episode :2220, avg score : 395.2\n",
            "# of episode :2240, avg score : 230.2\n",
            "# of episode :2260, avg score : 243.3\n",
            "# of episode :2280, avg score : 308.5\n",
            "# of episode :2300, avg score : 288.4\n",
            "# of episode :2320, avg score : 259.9\n",
            "# of episode :2340, avg score : 271.6\n",
            "# of episode :2360, avg score : 295.4\n",
            "# of episode :2380, avg score : 267.1\n",
            "# of episode :2400, avg score : 227.2\n",
            "# of episode :2420, avg score : 229.2\n",
            "# of episode :2440, avg score : 236.6\n",
            "# of episode :2460, avg score : 256.4\n",
            "# of episode :2480, avg score : 240.7\n",
            "# of episode :2500, avg score : 241.8\n",
            "# of episode :2520, avg score : 254.1\n",
            "# of episode :2540, avg score : 320.1\n",
            "# of episode :2560, avg score : 251.5\n",
            "# of episode :2580, avg score : 208.8\n",
            "# of episode :2600, avg score : 198.8\n",
            "# of episode :2620, avg score : 192.4\n",
            "# of episode :2640, avg score : 199.8\n",
            "# of episode :2660, avg score : 228.2\n",
            "# of episode :2680, avg score : 333.1\n",
            "# of episode :2700, avg score : 333.9\n",
            "# of episode :2720, avg score : 299.4\n",
            "# of episode :2740, avg score : 335.9\n",
            "# of episode :2760, avg score : 302.2\n",
            "# of episode :2780, avg score : 322.6\n",
            "# of episode :2800, avg score : 329.4\n",
            "# of episode :2820, avg score : 309.4\n",
            "# of episode :2840, avg score : 320.9\n",
            "# of episode :2860, avg score : 324.6\n",
            "# of episode :2880, avg score : 277.9\n",
            "# of episode :2900, avg score : 309.9\n",
            "# of episode :2920, avg score : 320.6\n",
            "# of episode :2940, avg score : 325.1\n",
            "# of episode :2960, avg score : 361.7\n",
            "# of episode :2980, avg score : 308.1\n",
            "# of episode :3000, avg score : 280.5\n",
            "# of episode :3020, avg score : 248.2\n",
            "# of episode :3040, avg score : 282.6\n",
            "# of episode :3060, avg score : 290.9\n",
            "# of episode :3080, avg score : 318.5\n",
            "# of episode :3100, avg score : 286.8\n",
            "# of episode :3120, avg score : 340.1\n",
            "# of episode :3140, avg score : 306.0\n",
            "# of episode :3160, avg score : 289.2\n",
            "# of episode :3180, avg score : 215.3\n",
            "# of episode :3200, avg score : 177.7\n",
            "# of episode :3220, avg score : 173.5\n",
            "# of episode :3240, avg score : 169.2\n",
            "# of episode :3260, avg score : 176.2\n",
            "# of episode :3280, avg score : 170.4\n",
            "# of episode :3300, avg score : 170.0\n",
            "# of episode :3320, avg score : 168.2\n",
            "# of episode :3340, avg score : 169.0\n",
            "# of episode :3360, avg score : 162.8\n",
            "# of episode :3380, avg score : 175.8\n",
            "# of episode :3400, avg score : 186.9\n",
            "# of episode :3420, avg score : 179.8\n",
            "# of episode :3440, avg score : 171.2\n",
            "# of episode :3460, avg score : 168.3\n",
            "# of episode :3480, avg score : 165.7\n",
            "# of episode :3500, avg score : 172.4\n",
            "# of episode :3520, avg score : 159.8\n",
            "# of episode :3540, avg score : 159.8\n",
            "# of episode :3560, avg score : 166.8\n",
            "# of episode :3580, avg score : 174.2\n",
            "# of episode :3600, avg score : 187.1\n",
            "# of episode :3620, avg score : 298.0\n",
            "# of episode :3640, avg score : 312.5\n",
            "# of episode :3660, avg score : 369.4\n",
            "# of episode :3680, avg score : 371.4\n",
            "# of episode :3700, avg score : 373.5\n",
            "# of episode :3720, avg score : 353.2\n",
            "# of episode :3740, avg score : 268.9\n",
            "# of episode :3760, avg score : 254.4\n",
            "# of episode :3780, avg score : 279.4\n",
            "# of episode :3800, avg score : 299.1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-efc71599e668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-efc71599e668>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mforce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce_mag\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce_mag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mcostheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0msintheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "#Hyperparameters\n",
        "learning_rate = 0.0002\n",
        "gamma         = 0.98\n",
        "n_rollout     = 10\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.data = []\n",
        "        \n",
        "        self.fc1 = nn.Linear(4,256)\n",
        "        self.fc_pi = nn.Linear(256,2)\n",
        "        self.fc_v = nn.Linear(256,1)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        \n",
        "    def pi(self, x, softmax_dim = 0):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc_pi(x)\n",
        "        prob = F.softmax(x, dim=softmax_dim)\n",
        "        return prob\n",
        "    \n",
        "    def v(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        v = self.fc_v(x)\n",
        "        return v\n",
        "    \n",
        "    def put_data(self, transition):\n",
        "        self.data.append(transition)\n",
        "        \n",
        "    def make_batch(self):\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
        "        for transition in self.data:\n",
        "            s,a,r,s_prime,done = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r/100.0])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            done_lst.append([done_mask])\n",
        "        \n",
        "        s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
        "                                                               torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
        "                                                               torch.tensor(done_lst, dtype=torch.float)\n",
        "        self.data = []\n",
        "        return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n",
        "  \n",
        "    def train_net(self):\n",
        "        s, a, r, s_prime, done = self.make_batch()\n",
        "        td_target = r + gamma * self.v(s_prime) * done\n",
        "        delta = td_target - self.v(s)\n",
        "        \n",
        "        pi = self.pi(s, softmax_dim=1)\n",
        "        pi_a = pi.gather(1,a)\n",
        "        loss = -torch.log(pi_a) * delta.detach() + F.smooth_l1_loss(self.v(s), td_target.detach())\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.mean().backward()\n",
        "        self.optimizer.step()         \n",
        "      \n",
        "def main():  \n",
        "    env = gym.make('CartPole-v1')\n",
        "    model = ActorCritic()    \n",
        "    print_interval = 20\n",
        "    score = 0.0\n",
        "\n",
        "    for n_epi in range(10000):\n",
        "        done = False\n",
        "        s = env.reset()\n",
        "        while not done:\n",
        "            for t in range(n_rollout):\n",
        "                prob = model.pi(torch.from_numpy(s).float())\n",
        "                m = Categorical(prob)\n",
        "                a = m.sample().item()\n",
        "                s_prime, r, done, info = env.step(a)\n",
        "                model.put_data((s,a,r,s_prime,done))\n",
        "                \n",
        "                s = s_prime\n",
        "                score += r\n",
        "                \n",
        "                if done:\n",
        "                    break                     \n",
        "            \n",
        "            model.train_net()\n",
        "            \n",
        "        if n_epi%print_interval==0 and n_epi!=0:\n",
        "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
        "            score = 0.0\n",
        "    env.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}