{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AWS_Reinforcement_Learning/blob/main/MCandTD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHHxJ9b84YZP",
        "outputId": "04f650b9-10c1-4d45-ea75-302dc7146ddd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x=0\n",
        "        self.y=0\n",
        "    \n",
        "    def step(self, a):\n",
        "        if a==0:\n",
        "            self.move_right()\n",
        "        elif a==1:\n",
        "            self.move_left()\n",
        "        elif a==2:\n",
        "            self.move_up()\n",
        "        elif a==3:\n",
        "            self.move_down()\n",
        "\n",
        "        reward = -1\n",
        "        done = self.is_done()\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    def move_right(self):\n",
        "        self.y += 1  \n",
        "        if self.y > 3:\n",
        "            self.y = 3\n",
        "      \n",
        "    def move_left(self):\n",
        "        self.y -= 1\n",
        "        if self.y < 0:\n",
        "            self.y = 0\n",
        "      \n",
        "    def move_up(self):\n",
        "        self.x -= 1\n",
        "        if self.x < 0:\n",
        "            self.x = 0\n",
        "  \n",
        "    def move_down(self):\n",
        "        self.x += 1\n",
        "        if self.x > 3:\n",
        "            self.x = 3\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x == 3 and self.y == 3:\n",
        "            return True\n",
        "        else :\n",
        "            return False\n",
        "\n",
        "    def get_state(self):\n",
        "        return (self.x, self.y)\n",
        "      \n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self):\n",
        "        pass        \n",
        "\n",
        "    def select_action(self):\n",
        "        coin = random.random()\n",
        "        if coin < 0.25:\n",
        "            action = 0\n",
        "        elif coin < 0.5:\n",
        "            action = 1\n",
        "        elif coin < 0.75:\n",
        "            action = 2\n",
        "        else:\n",
        "            action = 3\n",
        "        return action\n",
        "\n",
        "#MC\n",
        "\n",
        "def main():\n",
        "    env = GridWorld()\n",
        "    agent = Agent()\n",
        "    data = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]] # 테이블 초기화\n",
        "    gamma = 1.0\n",
        "    alpha = 0.0001\n",
        "\n",
        "    for k in range(50000): # 총 5만번의 에피소드 진행\n",
        "        done = False\n",
        "        history = []\n",
        "        while not done:\n",
        "            action = agent.select_action()\n",
        "            (x,y), reward, done = env.step(action)\n",
        "            history.append((x,y,reward))\n",
        "        env.reset()\n",
        "\n",
        "        # 매 에피소드가 끝나고 바로 해당 데이터를 이용해 테이블을 업데이트\n",
        "        cum_reward = 0\n",
        "        for transition in history[::-1]:\n",
        "            x, y, reward = transition\n",
        "            data[x][y] = data[x][y] + alpha*(cum_reward-data[x][y])\n",
        "            cum_reward = cum_reward + gamma*reward\n",
        "            \n",
        "    #학습이 끝나고 난 후 데이터를 출력해보기 위한 코드\n",
        "    np.set_printoptions(precision=2)\n",
        "    np.set_printoptions(suppress=True)\n",
        "    print(np.array(data))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-59.72 -57.65 -54.02 -51.27]\n",
            " [-57.08 -54.35 -49.6  -44.73]\n",
            " [-54.1  -49.74 -40.3  -29.39]\n",
            " [-51.27 -45.61 -29.94   0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtysat0KLYWp",
        "outputId": "e46fa6a6-7332-4156-e2da-09c1df1bed10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def main():\n",
        "    env = GridWorld()\n",
        "    agent = Agent()\n",
        "    data = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
        "    gamma = 1.0\n",
        "    alpha = 0.01\n",
        "\n",
        "    for k in range(50000): # 총 5만번의 에피소드 진행\n",
        "        done = False\n",
        "        while not done:\n",
        "            x, y = env.get_state()\n",
        "            action = agent.select_action()\n",
        "            (x_prime, y_prime), reward, done = env.step(action)\n",
        "            x_prime, y_prime = env.get_state()\n",
        "\n",
        "            # 한 번의 step이 진행되자마자 바로 테이블의 데이터를 업데이트 해줌\n",
        "            data[x][y] = data[x][y] + alpha*(reward+gamma*data[x_prime][y_prime]-data[x][y])\n",
        "        env.reset()\n",
        "\n",
        "    #학습이 끝나고 난 후 데이터를 출력해보기 위한 코드\n",
        "    np.set_printoptions(precision=2)\n",
        "    np.set_printoptions(suppress=True)\n",
        "    print(np.array(data))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-59.41 -57.46 -54.6  -51.64]\n",
            " [-57.32 -54.42 -50.38 -45.93]\n",
            " [-54.14 -49.23 -41.02 -30.24]\n",
            " [-51.64 -44.16 -29.62   0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh3v4mEg3zC2",
        "outputId": "7447898c-6438-48d3-bf5f-79be9d656e09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = np.random.rand(5,3,3)\n",
        "\n",
        "for row in x:\n",
        "  for col in row:\n",
        "    print(col)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.97 0.81 0.47]\n",
            "[0.21 0.74 0.41]\n",
            "[0.35 0.29 0.66]\n",
            "[0.85 0.47 0.66]\n",
            "[0.77 0.34 0.66]\n",
            "[0.2  0.9  0.62]\n",
            "[0.81 0.86 0.76]\n",
            "[0.64 0.69 0.72]\n",
            "[0.09 0.45 0.85]\n",
            "[0.84 0.02 0.68]\n",
            "[0.9  0.84 0.06]\n",
            "[0.01 0.57 0.9 ]\n",
            "[0.6  0.   0.04]\n",
            "[0.85 0.37 0.83]\n",
            "[0.84 0.16 0.6 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azt0v7hA3Jl_",
        "outputId": "9ec8e56c-c7c5-423b-ae5c-e2e9addd229b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "data = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
        "env = GridWorld()\n",
        "gamma = 1.0\n",
        "\n",
        "for k in range(10000):\n",
        "    env.initialize()\n",
        "    while not env.done():\n",
        "        x, y = env.x, env.y\n",
        "        env.move_random()\n",
        "        history = env.get_history()\n",
        "        next_x, next_y, reward = history[-1]\n",
        "        data[x][y] = 0.99*data[x][y] + 0.01*(reward+data[next_x][next_y])\n",
        "        \n",
        "    env.initialize()\n",
        "        \n",
        "for row in data:\n",
        "    print(row)\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-38ef3b0bf680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GridWorld' object has no attribute 'initialize'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHDeaBw-Jw10",
        "outputId": "ca8fce5c-9506-4239-b223-8cee17a746d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import copy \n",
        "\n",
        "data = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
        "\n",
        "\n",
        "\n",
        "for step in range(6):\n",
        "\tdata_temp = copy.deepcopy(data)\n",
        "\tfor i in range(4):\n",
        "\t\tfor j in range(4):\n",
        "\t\t\tleft = i, max(j-1,0)\n",
        "\t\t\tright = i, min(j+1,3)\n",
        "\t\t\tup = max(i-1,0), j\n",
        "\t\t\tdown = min(i+1,3), j\n",
        "\t\t\tif i==3 and j==3:\n",
        "\t\t\t\tpass\n",
        "\t\t\telse:\n",
        "\t\t\t\tdata[i][j] = 0.25*(-1+data_temp[left[0]][left[1]]) + 0.25*(-1+data_temp[right[0]][right[1]]) +\\\n",
        "\t\t\t\t\t\t\t       0.25*(-1+data_temp[up[0]][up[1]])+ 0.25*(-1+data_temp[down[0]][down[1]])\n",
        "\n",
        "\n",
        "print(data)\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-6.0, -5.990234375, -5.95703125, -5.900390625], [-5.990234375, -5.9453125, -5.794921875, -5.552734375], [-5.95703125, -5.794921875, -5.2734375, -4.21875], [-5.900390625, -5.552734375, -4.21875, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4q38Zi0J1st",
        "outputId": "638a05d3-ca1f-4a88-cba1-7b69849a4fac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x=0\n",
        "        self.y=0\n",
        "    \n",
        "    def step(self, a):\n",
        "\t      # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n",
        "        if a==0:\n",
        "            self.move_left()\n",
        "        elif a==1:\n",
        "            self.move_up()\n",
        "        elif a==2:\n",
        "            self.move_right()\n",
        "        elif a==3:\n",
        "            self.move_down()\n",
        "\n",
        "        reward = -1  # 보상은 항상 -1로 고정\n",
        "        done = self.is_done()\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    def move_left(self):\n",
        "        if self.y==0:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==5 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        else:\n",
        "            self.y -= 1\n",
        "\n",
        "    def move_right(self):\n",
        "        if self.y==1 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        elif self.y==6:\n",
        "            pass\n",
        "        else:\n",
        "            self.y += 1\n",
        "      \n",
        "    def move_up(self):\n",
        "        if self.x==0:\n",
        "            pass\n",
        "        elif self.x==3 and self.y==2:\n",
        "            pass\n",
        "        else:\n",
        "            self.x -= 1\n",
        "\n",
        "    def move_down(self):\n",
        "        if self.x==4:\n",
        "            pass\n",
        "        elif self.x==1 and self.y==4:\n",
        "            pass\n",
        "        else:\n",
        "            self.x+=1\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x==4 and self.y==6: # 목표 지점인 (4,6)에 도달하면 끝난다\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "      \n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)\n",
        "\n",
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        self.q_table = np.zeros((5, 7, 4)) # q벨류를 저장하는 변수. 모두 0으로 초기화. \n",
        "        self.eps = 0.9 \n",
        "        self.alpha = 0.01\n",
        "        \n",
        "    def select_action(self, s):\n",
        "        # eps-greedy로 액션을 선택해준다\n",
        "        x, y = s\n",
        "        coin = random.random()\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0,3)\n",
        "        else:\n",
        "            action_val = self.q_table[x,y,:]\n",
        "            action = np.argmax(action_val)\n",
        "        return action\n",
        "\n",
        "    def update_table(self, history):\n",
        "        # 한 에피소드에 해당하는 history를 입력으로 받아 q 테이블의 값을 업데이트 한다\n",
        "        cum_reward = 0\n",
        "        for transition in history[::-1]:\n",
        "            s, a, r, s_prime = transition\n",
        "            x,y = s\n",
        "            # 몬테 카를로 방식을 이용하여 업데이트.\n",
        "            self.q_table[x,y,a] = self.q_table[x,y,a] + self.alpha * (cum_reward - self.q_table[x,y,a])\n",
        "            cum_reward = cum_reward + r \n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.03\n",
        "        self.eps = max(self.eps, 0.1)\n",
        "\n",
        "    def show_table(self):\n",
        "        # 학습이 각 위치에서 어느 액션의 q 값이 가장 높았는지 보여주는 함수\n",
        "        q_lst = self.q_table.tolist()\n",
        "        data = np.zeros((5,7))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)\n",
        "\n",
        "\n",
        "def main():\n",
        "    env = GridWorld()\n",
        "    agent = QAgent()\n",
        "\n",
        "    for n_epi in range(1000): # 총 1,000 에피소드 동안 학습\n",
        "        done = False\n",
        "        history = []\n",
        "\n",
        "        s = env.reset()\n",
        "        n_step = 0\n",
        "        while not done: # 한 에피소드가 끝날 때 까지\n",
        "            a = agent.select_action(s)\n",
        "            s_prime, r, done = env.step(a)\n",
        "            history.append((s, a, r, s_prime))\n",
        "            s = s_prime\n",
        "            n_step += 1\n",
        "        agent.update_table(history) # 히스토리를 이용하여 에이전트를 업데이트\n",
        "        agent.anneal_eps()\n",
        "\n",
        "    agent.show_table() # 학습이 끝난 결과를 출력\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3. 3. 0. 3. 3. 3. 2.]\n",
            " [3. 2. 0. 2. 2. 3. 3.]\n",
            " [2. 3. 0. 1. 0. 3. 3.]\n",
            " [3. 3. 0. 1. 0. 3. 3.]\n",
            " [2. 2. 2. 1. 0. 2. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8VPbZ7vJbHz",
        "outputId": "6124ce81-7ff9-417a-a8a7-7d614bddef3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##sarsa\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x=0\n",
        "        self.y=0\n",
        "    \n",
        "    def step(self, a):\n",
        "          # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n",
        "        if a==0:\n",
        "            self.move_left()\n",
        "        elif a==1:\n",
        "            self.move_up()\n",
        "        elif a==2:\n",
        "            self.move_right()\n",
        "        elif a==3:\n",
        "            self.move_down()\n",
        "\n",
        "        reward = -1  # 보상은 항상 -1로 고정\n",
        "        done = self.is_done()\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    def move_left(self):\n",
        "        if self.y==0:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==5 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        else:\n",
        "            self.y -= 1\n",
        "\n",
        "    def move_right(self):\n",
        "        if self.y==1 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        elif self.y==6:\n",
        "            pass\n",
        "        else:\n",
        "            self.y += 1\n",
        "      \n",
        "    def move_up(self):\n",
        "        if self.x==0:\n",
        "            pass\n",
        "        elif self.x==3 and self.y==2:\n",
        "            pass\n",
        "        else:\n",
        "            self.x -= 1\n",
        "\n",
        "    def move_down(self):\n",
        "        if self.x==4:\n",
        "            pass\n",
        "        elif self.x==1 and self.y==4:\n",
        "            pass\n",
        "        else:\n",
        "            self.x+=1\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x==4 and self.y==6: # 목표 지점인 (4,6)에 도달하면 끝난다\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "      \n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)\n",
        "\n",
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        self.q_table = np.zeros((5, 7, 4)) # 마찬가지로 Q 테이블을 0으로 초기화\n",
        "        self.eps = 0.9\n",
        "\n",
        "    def select_action(self, s):\n",
        "        x, y = s\n",
        "        coin = random.random()\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0,3)\n",
        "        else:\n",
        "            action_val = self.q_table[x,y,:]\n",
        "            action = np.argmax(action_val)\n",
        "        return action\n",
        "\n",
        "    def update_table(self, transition):\n",
        "        s, a, r, s_prime = transition\n",
        "        x,y = s\n",
        "        next_x, next_y = s_prime\n",
        "        a_prime = self.select_action(s_prime) # S'에서 선택할 액션 (실제로 취한 액션이 아님)\n",
        "        # SARSA 업데이트 식을 이용\n",
        "        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1 * (r + self.q_table[next_x,next_y,a_prime] - self.q_table[x,y,a])\n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.03\n",
        "        self.eps = max(self.eps, 0.1)\n",
        "\n",
        "    def show_table(self):\n",
        "        q_lst = self.q_table.tolist()\n",
        "        data = np.zeros((5,7))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)\n",
        "\n",
        "      \n",
        "\n",
        "def main():\n",
        "    env = GridWorld()\n",
        "    agent = QAgent()\n",
        "\n",
        "    for n_epi in range(1000):\n",
        "        done = False\n",
        "\n",
        "        s = env.reset()\n",
        "        while not done:\n",
        "            a = agent.select_action(s)\n",
        "            s_prime, r, done = env.step(a)\n",
        "            agent.update_table((s,a,r,s_prime))\n",
        "            s = s_prime\n",
        "        agent.anneal_eps()\n",
        "        \n",
        "    agent.show_table()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3. 3. 0. 2. 1. 2. 3.]\n",
            " [3. 3. 0. 2. 2. 2. 3.]\n",
            " [2. 3. 0. 1. 0. 3. 3.]\n",
            " [2. 2. 2. 1. 0. 3. 3.]\n",
            " [0. 2. 2. 1. 0. 2. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8lQU0XzJyjo",
        "outputId": "b01fd657-fd6e-449b-962e-651b932ad428",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#q러닝\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x=0\n",
        "        self.y=0\n",
        "    \n",
        "    def step(self, a):\n",
        "        if a==0:\n",
        "            self.move_left()\n",
        "        elif a==1:\n",
        "            self.move_up()\n",
        "        elif a==2:\n",
        "            self.move_right()\n",
        "        elif a==3:\n",
        "            self.move_down()\n",
        "\n",
        "        reward = -1\n",
        "        done = self.is_done()\n",
        "        return (self.x, self.y), reward, done\n",
        "\n",
        "    def move_left(self):\n",
        "        if self.y==0:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==5 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        else:\n",
        "            self.y -= 1\n",
        "\n",
        "    def move_right(self):\n",
        "        if self.y==1 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        elif self.y==6:\n",
        "            pass\n",
        "        else:\n",
        "            self.y += 1\n",
        "      \n",
        "    def move_up(self):\n",
        "        if self.x==0:\n",
        "            pass\n",
        "        elif self.x==3 and self.y==2:\n",
        "            pass\n",
        "        else:\n",
        "            self.x -= 1\n",
        "\n",
        "    def move_down(self):\n",
        "        if self.x==4:\n",
        "            pass\n",
        "        elif self.x==1 and self.y==4:\n",
        "            pass\n",
        "        else:\n",
        "            self.x+=1\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x==4 and self.y==6:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "      \n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)\n",
        "\n",
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        self.q_table = np.zeros((5, 7, 4)) # 마찬가지로 Q 테이블을 0으로 초기화\n",
        "        self.eps = 0.9\n",
        "\n",
        "    def select_action(self, s):\n",
        "        # eps-greedy로 액션을 선택해준다\n",
        "        x, y = s\n",
        "        coin = random.random()\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0,3)\n",
        "        else:\n",
        "            action_val = self.q_table[x,y,:]\n",
        "            action = np.argmax(action_val)\n",
        "        return action\n",
        "\n",
        "    def update_table(self, transition):\n",
        "        s, a, r, s_prime = transition\n",
        "        x,y = s\n",
        "        next_x, next_y = s_prime\n",
        "        # Q러닝 업데이트 식을 이용 \n",
        "        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1 * (r + np.amax(self.q_table[next_x,next_y,:]) - self.q_table[x,y,a])\n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.01  # Q러닝에선 epsilon 이 좀더 천천히 줄어 들도록 함.\n",
        "        self.eps = max(self.eps, 0.2) \n",
        "\n",
        "    def show_table(self):\n",
        "        q_lst = self.q_table.tolist()\n",
        "        data = np.zeros((5,7))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)\n",
        "      \n",
        "\n",
        "def main():\n",
        "    env = GridWorld()\n",
        "    agent = QAgent()\n",
        "\n",
        "    for n_epi in range(1000):\n",
        "        done = False\n",
        "\n",
        "        s = env.reset()\n",
        "        while not done:\n",
        "            a = agent.select_action(s)\n",
        "            s_prime, r, done = env.step(a)\n",
        "            agent.update_table((s,a,r,s_prime))\n",
        "            s = s_prime\n",
        "        agent.anneal_eps()\n",
        "\n",
        "    agent.show_table()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2. 3. 0. 2. 2. 2. 3.]\n",
            " [3. 3. 0. 2. 2. 3. 3.]\n",
            " [3. 3. 0. 1. 0. 3. 3.]\n",
            " [2. 2. 2. 1. 0. 2. 3.]\n",
            " [2. 1. 1. 1. 0. 2. 0.]]\n"
          ]
        }
      ]
    }
  ]
}